# -*- coding: utf-8 -*-
"""Proyek Klasifikasi Gambar

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l06wPtkJGvNfYuPeLYEl6Hp-TBok0hw6

# Proyek Klasifikasi Gambar: [Vegetable Image Dataset]
- **Nama:** Rebecca Olivia Javenka Br. Manurung
- **Email:** [rebeccaolivia1601@gmail.com]
- **ID Dicoding:** [rebeccaolivia]

## Import Semua Packages/Library yang Digunakan
"""

import os
import shutil
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from tensorflow.keras.utils import image_dataset_from_directory

"""## Data Preparation

### Data Loading
"""

import kagglehub

# Download dataset dari Kaggle
path = kagglehub.dataset_download("misrakahmed/vegetable-image-dataset")
print("Path to dataset files:", path)

# Path utama dataset
base_path = os.path.join(path, "Vegetable Images")

# Path dataset asli
original_base = "/kaggle/input/vegetable-image-dataset/Vegetable Images"
combined_dir = "/kaggle/working/combined_vegetables"

# Buat folder baru untuk data gabungan
os.makedirs(combined_dir, exist_ok=True)

# Loop untuk menggabungkan semua gambar
for split in ['train', 'validation', 'test']:
    split_path = os.path.join(original_base, split)
    for class_name in os.listdir(split_path):
        class_path = os.path.join(split_path, class_name)
        dest_path = os.path.join(combined_dir, class_name)
        os.makedirs(dest_path, exist_ok=True)

        # Salin semua gambar ke folder baru
        for img_file in os.listdir(class_path):
            src = os.path.join(class_path, img_file)
            dst = os.path.join(dest_path, f"{split}_{img_file}")
            shutil.copy(src, dst)

print("âœ… Semua data telah digabungkan!")

base_path = "/kaggle/working/manual_split"
all_classes = os.listdir(combined_dir)

# Buat folder untuk train/val/test
for split in ['train', 'val', 'test']:
    os.makedirs(os.path.join(base_path, split), exist_ok=True)
    for class_name in all_classes:
        os.makedirs(os.path.join(base_path, split, class_name), exist_ok=True)

# Bagi setiap kelas secara proporsional
for class_name in all_classes:
    class_path = os.path.join(combined_dir, class_name)
    all_images = [f for f in os.listdir(class_path) if f.endswith('.jpg')]

    # Split 70% train, 15% val, 15% test
    train, temp = train_test_split(all_images, test_size=0.3, random_state=42)
    val, test = train_test_split(temp, test_size=0.5, random_state=42)

    # Salin ke folder baru
    for img in train:
        shutil.copy(os.path.join(class_path, img), os.path.join(base_path, 'train', class_name, img))
    for img in val:
        shutil.copy(os.path.join(class_path, img), os.path.join(base_path, 'val', class_name, img))
    for img in test:
        shutil.copy(os.path.join(class_path, img), os.path.join(base_path, 'test', class_name, img))

print("âœ… Dataset telah dibagi secara manual!")

print("Jumlah gambar per kelas di setiap subset:")
for split in ['train', 'val', 'test']:
    print(f"\nðŸ”¹ {split.upper()}:")
    for class_name in all_classes:
        count = len(os.listdir(os.path.join(base_path, split, class_name)))
        print(f"{class_name}: {count} gambar")

# Load data training
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(base_path, 'train'),
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical'
)

# Load data validasi
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(base_path, 'val'),
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical'
)

# Load data testing (tidak di-shuffle agar konsisten)
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(base_path, 'test'),
    image_size=(224, 224),
    batch_size=32,
    shuffle=False,
    label_mode='categorical'
)

# Dapatkan nama kelas
class_names = train_ds.class_names
print("\nKelas yang terdeteksi:", class_names)

"""### Data Preprocessing"""

# Normalisasi pixel ke range [0,1]
normalization_layer = tf.keras.layers.Rescaling(1./255)
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))
test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))

# Optimasi dengan prefetch
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

"""## Modelling"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization

model = tf.keras.Sequential([
    # Blok Conv 1
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.BatchNormalization(),

    # Blok Conv 2
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.BatchNormalization(),

    # Blok Conv 3
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.BatchNormalization(),

    # Fully Connected
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(len(class_names), activation='softmax')  # 15 kelas
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Callbacks
early_stop = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5)

# Training
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    callbacks=[early_stop, lr_reduce]
)

"""## Evaluasi dan Visualisasi"""

import matplotlib.pyplot as plt

# Ambil data history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, len(acc) + 1)

# Plot akurasi
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""## Konversi Model"""

import os
import tensorflow as tf

# Buat folder utama untuk semua model
model_dir = "model_vegetable_classification"
os.makedirs(model_dir, exist_ok=True)

"""### Simpan model dalam format SavedModel

"""

saved_model_dir = os.path.join(model_dir, "saved_model")
model.export(saved_model_dir)  # Gunakan export() untuk format SavedModel

"""### Konversi ke TFLite"""

tflite_dir = os.path.join(model_dir, "tflite")
os.makedirs(tflite_dir, exist_ok=True)

tflite_path = os.path.join(tflite_dir, "model.tflite")
labels_path = os.path.join(tflite_dir, "labels.txt")

# Konversi dari SavedModel
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

# Simpan model + label
with open(tflite_path, 'wb') as f:
    f.write(tflite_model)

with open(labels_path, 'w') as f:
    f.write("\n".join(class_names))  # class_names dari training

"""### Konversi ke TensorFlow.js"""

tfjs_dir = os.path.join(model_dir, "tfjs")
os.makedirs(tfjs_dir, exist_ok=True)

# Install library dan konversi
!pip install tensorflowjs
!tensorflowjs_converter \
    --input_format=tf_saved_model \
    --output_format=tfjs_graph_model \
    {saved_model_dir} \
    {tfjs_dir}

"""### Download Model"""

import shutil
shutil.make_archive("model_vegetable_classification", 'zip', "model_vegetable_classification")

from google.colab import files
files.download("model_vegetable_classification.zip")

"""## Inference (Optional)"""

import os
import random
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# 1. Path ke model dan data
MODEL_DIR = "model_vegetable_classification/saved_model"  # Folder SavedModel
TEST_DATA_DIR = "/kaggle/working/manual_split/test"      # Folder test hasil splitting manual
LABELS_PATH = "model_vegetable_classification/tflite/labels.txt"  # File label

# 2. Load SavedModel
model = tf.saved_model.load(MODEL_DIR)
inference_fn = model.signatures["serving_default"]  # Fungsi inference

# 3. Load class names
with open(LABELS_PATH, "r") as f:
    class_names = [line.strip() for line in f.readlines()]

# 4. Ambil gambar dari folder test (hasil splitting manual)
def get_test_images(num_samples=10):
    image_paths = []
    for root, _, files in os.walk(TEST_DATA_DIR):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                image_paths.append(os.path.join(root, file))

    return random.sample(image_paths, min(num_samples, len(image_paths)))

# 5. Prediksi dan visualisasi
def run_inference(image_paths):
    plt.figure(figsize=(20, 8))

    for i, img_path in enumerate(image_paths):
        # Preprocessing
        img = load_img(img_path, target_size=(224, 224))
        img_array = img_to_array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0).astype(np.float32)

        # Inference
        pred = inference_fn(tf.constant(img_array))
        scores = tf.nn.softmax(pred['output_0']).numpy()[0]
        predicted_class = class_names[np.argmax(scores)]
        confidence = np.max(scores) * 100

        # Plot
        plt.subplot(2, 5, i+1)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"{predicted_class}\n{confidence:.1f}%")

    plt.tight_layout()
    plt.show()

    # Print detail
    for img_path in image_paths:
        img = load_img(img_path, target_size=(224, 224))
        img_array = img_to_array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0).astype(np.float32)

        pred = inference_fn(tf.constant(img_array))
        scores = tf.nn.softmax(pred['output_0']).numpy()[0]

        print(f"\nGambar: {os.path.basename(img_path)}")
        print("Top 3 Prediksi:")
        top3_indices = np.argsort(scores)[::-1][:3]
        for idx in top3_indices:
            print(f"  {class_names[idx]:20s}: {scores[idx]*100:.2f}%")

# 6. Jalankan inference
test_images = get_test_images(num_samples=10)
if not test_images:
    print("Error: Tidak ada gambar ditemukan di", TEST_DATA_DIR)
    print("Pastikan:")
    print("1. Anda sudah menjalankan pembagian dataset manual")
    print("2. Folder test ada di:", TEST_DATA_DIR)
else:
    run_inference(test_images)

!pip freeze > requirements.txt
print("requirements.txt generated!")