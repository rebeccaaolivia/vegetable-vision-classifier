# -*- coding: utf-8 -*-
"""Proyek Klasifikasi Gambar Dicoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ks_cubAqpHb7PJkKiuoskQgKvIQa5V_b

# Proyek Klasifikasi Gambar: [Vegetable Image Dataset]
- **Nama:** Rebecca Olivia Javenka Br. Manurung
- **Email:** [rebeccaolivia1601@gmail.com]
- **ID Dicoding:** [rebeccaolivia]

## Import Semua Packages/Library yang Digunakan
"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
import os

"""## Data Preparation

### Data Loading
"""

import kagglehub

# Download dataset dari Kaggle
path = kagglehub.dataset_download("misrakahmed/vegetable-image-dataset")
print("Path to dataset files:", path)

# Path utama dataset
base_path = os.path.join(path, "Vegetable Images")

# Load training data (tidak perlu validation_split karena sudah ada folder sendiri)
train_ds = image_dataset_from_directory(
    os.path.join(base_path, "train"),
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical'
)

# Load validation data
val_ds = image_dataset_from_directory(
    os.path.join(base_path, "validation"),
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical'
)

# Load test data
test_ds = image_dataset_from_directory(
    os.path.join(base_path, "test"),
    image_size=(224, 224),
    batch_size=32,
    shuffle=False,  # supaya urutan sama untuk evaluasi
    label_mode='categorical'
)

# Get class names BEFORE prefetching
class_names = train_ds.class_names
print("Kelas yang terdeteksi:", class_names)

# Optimasi performa dengan prefetch

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

"""### Data Preprocessing"""

# Normalisasi pixel ke range [0,1]
normalization_layer = tf.keras.layers.Rescaling(1./255)

train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))
test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization

"""## Modelling"""

# Membangun model CNN
model = Sequential([
    # Conv block 1
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    BatchNormalization(),

    # Conv block 2
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    BatchNormalization(),

    # Conv block 3
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    BatchNormalization(),

    # Fully connected
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(15, activation='softmax')  # 15 kelas sayuran
])

# Compile model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

"""Callbacks untuk early stopping dan learning rate reduction dan Training model

"""

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Callbacks untuk early stopping dan learning rate reduction
early_stop = EarlyStopping(patience=5, restore_best_weights=True)
lr_reduce = ReduceLROnPlateau(patience=3, factor=0.5)

# Training model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    callbacks=[early_stop, lr_reduce]
)

"""## Evaluasi dan Visualisasi"""

import matplotlib.pyplot as plt

# Ambil data history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, len(acc) + 1)

# Plot akurasi
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

# Evaluasi model pada test set
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""## Konversi Model

### Simpan model dalam format SavedModel
"""

saved_model_dir = 'saved_model_dir'
model.export(saved_model_dir)

"""### Konversi ke TFLite

"""

tflite_output_path = 'tflite/model.tflite'
label_output_path = 'tflite/label.txt'

os.makedirs(os.path.dirname(tflite_output_path), exist_ok=True)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

with open(tflite_output_path, 'wb') as f:
    f.write(tflite_model)

"""### Konversi ke TensorFlow.js"""

!pip install tensorflowjs
!tensorflowjs_converter \
    --input_format=tf_saved_model \
    --output_format=tfjs_graph_model \
    saved_model_dir \
tfjs_model

"""## Inference (Optional)"""

# Mengeksplorasi struktur direktori dataset

import os

# Menetapkan path dasar dataset
base_path = '/kaggle/input'

# Menelusuri seluruh struktur folder dan mencetak path-nya
# Ini membantu memahami bagaimana dataset diorganisir
for root, dirs, files in os.walk(base_path):
    print(root)

# Melakukan prediksi menggunakan model langsung
import os
import random
import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Path ke folder gambar test
path = '/kaggle/input/vegetable-image-dataset/Vegetable Images'
image_folder = os.path.join(path, 'test')  # atau 'validation'

# Mengumpulkan semua file gambar .jpg dari folder test
all_image_files = []
for root, dirs, files in os.walk(image_folder):
    for file in files:
        if file.lower().endswith('.jpg'):
            all_image_files.append(os.path.join(root, file))

print(f"Total gambar ditemukan: {len(all_image_files)}")

if len(all_image_files) == 0:
    print("Tidak ada gambar ditemukan.")
else:
    # Memilih 10 gambar acak (atau kurang jika tidak cukup)
    num_samples = min(10, len(all_image_files))
    random_image_files = random.sample(all_image_files, num_samples)

    # Membuat plot untuk menampilkan gambar dan prediksinya
    plt.figure(figsize=(20, 8))
    for i, img_path in enumerate(random_image_files):
        # Memuat dan memproses gambar
        img = image.load_img(img_path, target_size=(224, 224))
        img_array = image.img_to_array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        # Melakukan prediksi
        pred = model.predict(img_array)
        predicted_class = class_names[np.argmax(pred)]

        # Menampilkan gambar dan prediksinya
        plt.subplot(2, 5, i+1)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f'{predicted_class}\n{np.max(pred)*100:.1f}%')

    plt.tight_layout()
    plt.show()

# Melakukan prediksi menggunakan model yang disimpan (SavedModel format)
import os
import random
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.utils import load_img, img_to_array
from keras.layers import TFSMLayer

# Memuat model yang telah disimpan
saved_model_path = 'saved_model_dir'  # ganti sesuai lokasi model kamu
reloaded_layer = TFSMLayer(saved_model_path, call_endpoint='serving_default')

# Fungsi untuk melakukan prediksi menggunakan model yang dimuat ulang
def predict_with_saved_model(img_array):
    pred_dict = reloaded_layer(img_array)
    logits = pred_dict['output_0']
    return tf.nn.softmax(logits).numpy()

# Mendapatkan nama kelas (harus urutan sama dengan saat training)
class_names = sorted(os.listdir('/kaggle/input/vegetable-image-dataset/Vegetable Images/train'))

# Path ke folder test
image_folder = '/kaggle/input/vegetable-image-dataset/Vegetable Images/test'

# Mengumpulkan semua file gambar .jpg
all_image_files = []
for root, dirs, files in os.walk(image_folder):
    for file in files:
        if file.lower().endswith('.jpg'):
            all_image_files.append(os.path.join(root, file))

# Memilih 10 gambar acak untuk diprediksi
random_image_files = random.sample(all_image_files, 10)

# Visualisasi prediksi
plt.figure(figsize=(20, 8))

for i, img_path in enumerate(random_image_files):
    # Memuat dan memproses gambar
    img = load_img(img_path, target_size=(224, 224))
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # Melakukan prediksi
    pred = predict_with_saved_model(img_array)
    predicted_class = class_names[np.argmax(pred)]

    # Menampilkan gambar dan prediksinya
    plt.subplot(2, 5, i+1)
    plt.imshow(img)
    plt.axis('off')
    plt.title(f'{predicted_class}\n{np.max(pred)*100:.1f}%')

plt.tight_layout()
plt.show()

# Mencetak confidence score untuk setiap kelas pada setiap gambar
for i, img_path in enumerate(random_image_files):
    img = load_img(img_path, target_size=(224, 224))
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # Melakukan prediksi
    pred = predict_with_saved_model(img_array)
    predicted_class = class_names[np.argmax(pred)]

    # Mencetak hasil prediksi detail
    print(f'\nGambar ke-{i+1}: {os.path.basename(img_path)}')
    print(f'Prediksi: {predicted_class}')
    print("Confidence per class:")
    for j, label in enumerate(class_names):
        print(f"  {label:12s}: {pred[0][j]*100:.2f}%")

!pip freeze > requirements.txt
print("requirements.txt generated!")

